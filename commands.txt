sudo apt update
sudo apt upgrade

sudo apt install openjdk-11-jre-headless

sudo apt-get install ssh
# pdsh: Terminal Command: 
sudo apt-get install pdsh

ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys


export SPARK_HOME=/home/lavish/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export HADOOP_HOME=/home/lavish/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip="192.168.8.143"'
. ~/.bashrc

tar -xvf hadoop.tar.gz
tar -xvf spark.tgz

$HADOOP_HOME/bin/hdfs namenode -format

$HADOOP_HOME/sbin/start-dfs.sh

$SPARK_HOME/sbin/start-master.sh 

$SPARK_HOME/sbin/start-slave.sh spark://ubuntu:7077

pyspark

load data into HDFS
access it from PySpark and display
install cern db keras lib

hadoop fs -copyFromLocal /home/lavish/startUpFunding/data/* /user/lavish/data 

install pip 
pip3 install notebook
pip3 install jupyter

jupyter notebook --ip="192.168.8.136"

git config --global user.email "lavishthomas@gmail.com"
git config --global user.name "Lavish Thomas"

git clone git@github.com:lavishthomas/startupFunding.git
git pull origin master
git add *

git commit -m "first check in"
git push origin master


sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall
pip3 install --user --upgrade tensorflow  # install in $HOME
sudo pip3 install keras

