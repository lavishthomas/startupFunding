{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode,substring, length, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from itertools import cycle\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session starting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data to the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+---------------+\n",
      "|raised_amount_usd|year|       category|\n",
      "+-----------------+----+---------------+\n",
      "|            1.5E7|2014|     E-Commerce|\n",
      "|            1.4E7|2015|         Drones|\n",
      "|            1.4E7|2015|  Manufacturing|\n",
      "|        1870000.0|2011|             3D|\n",
      "|        1870000.0|2011|Computer Vision|\n",
      "+-----------------+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM investments\")\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQUERY =  \"\"\"\n",
    "            SELECT CATEGORY, \n",
    "            CAST(YEAR AS INT), \n",
    "            SUM(RAISED_AMOUNT_USD) AS TOTAL, \n",
    "            CAST(SUM(RAISED_AMOUNT_USD) AS DECIMAL(30)) AS TOTAL_DEC \n",
    "            FROM INVESTMENTS GROUP \n",
    "            BY CATEGORY, YEAR \n",
    "            \"\"\"\n",
    "#  ORDER BY YEAR DESC, TOTAL DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+--------------+----------+\n",
      "|          CATEGORY|YEAR|         TOTAL| TOTAL_DEC|\n",
      "+------------------+----+--------------+----------+\n",
      "|  Mobile Analytics|2010|       4.205E7|  42050000|\n",
      "|Big Data Analytics|2013|  2.35683979E9|2356839790|\n",
      "| Online Scheduling|2012|      9.5505E7|  95505000|\n",
      "|          Creative|2012|  1.65391911E8| 165391911|\n",
      "|              Apps|2008|6.3178257004E8| 631782570|\n",
      "+------------------+----+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(SQLQUERY)\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------+\n",
      "|          CATEGORY|FEATURES|         TOTAL|\n",
      "+------------------+--------+--------------+\n",
      "|  Mobile Analytics|[2010.0]|       4.205E7|\n",
      "|Big Data Analytics|[2013.0]|  2.35683979E9|\n",
      "| Online Scheduling|[2012.0]|      9.5505E7|\n",
      "|          Creative|[2012.0]|  1.65391911E8|\n",
      "|              Apps|[2008.0]|6.3178257004E8|\n",
      "+------------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['YEAR'], outputCol = 'FEATURES')\n",
    "featureDF = vectorAssembler.transform(sqlDF).select('CATEGORY', 'FEATURES', 'TOTAL')\n",
    "\n",
    "featureDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = featureDF.select('CATEGORY').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Categories = [row.CATEGORY for row in f.collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix to predict the amount for the Year 2020\n",
    "\n",
    "l =  [(2020,)]\n",
    "\n",
    "rdd = sc.parallelize(l)\n",
    "test = rdd.map(lambda x: Row(YEAR=x[0] ))\n",
    "testDF = sqlContext.createDataFrame(test)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['YEAR'], outputCol = 'FEATURES')\n",
    "vectorDF = vectorAssembler.transform(testDF).select('FEATURES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecating model training for each sector (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0 \n",
    "summaryInfo = []\n",
    "columns = ['Category', 'Gradient', 'Slope' ,'Intercept','RMSE', 'R2', 'Prediction']\n",
    "\n",
    "for category in Categories:\n",
    "    #print(category)\n",
    "    categoryDF=featureDF.filter(featureDF.CATEGORY == category)\n",
    "    if(categoryDF.count() > 10):\n",
    "        #count +=1\n",
    "        lr = LinearRegression(featuresCol = 'FEATURES', labelCol='TOTAL', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "        lr_model = lr.fit(categoryDF)        \n",
    "        if (lr_model.summary.r2 >= .5): \n",
    "            row=(category\n",
    "                 ,lr_model.coefficients\n",
    "                 ,float(lr_model.coefficients[0])\n",
    "                 ,lr_model.intercept\n",
    "                 ,lr_model.summary.rootMeanSquaredError\n",
    "                 ,lr_model.summary.r2\n",
    "                 ,lr_model.transform(vectorDF).take(1)[0].prediction )\n",
    "            summaryInfo.append(row)                        \n",
    "summaryDF = spark.createDataFrame(summaryInfo, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|            Category|            Gradient|               Slope|           Intercept|                RMSE|                R2|          Prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|    Personal Finance|[3.09531844548871...| 3.095318445488712E8|-6.20846579539486...|1.1403333171358914E9|0.5257188832329653|  4.40774644923291E9|\n",
      "|Reviews and Recom...|[9.38435051053559...| 9.384350510535592E7|-1.88076601008072...| 2.252600279853369E8|0.6748252913534014|1.4872793047460022E9|\n",
      "|         Health Care|[1.08986421814503...|1.0898642181450326E9|-2.17925088637620...|2.0237455217204065E9| 0.874761105659529|2.227483427676025...|\n",
      "|Application Perfo...|[1.46386390370929...| 1.463863903709295E8|-2.93690139320376...|4.7040592338191056E8|0.5668998218851247|2.0103692289013062E9|\n",
      "|        Credit Cards|[6.33419228608998...| 6.334192286089984E7|-1.27009681219964...|2.5974312104301742E8|0.5027780036421106| 9.410029590531769E8|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaryDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to an persistant table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDF.write.mode(\"overwrite\").saveAsTable(\"summaryDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|           Category|            Gradient|               Slope|           Intercept|                RMSE|                R2|          Prediction|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|      Biotechnology|[2.26309733431992...| 2.263097334319927E9|-4.52421180397470...| 5.284447122207029E9|0.8320372559570712| 4.72448113515459E10|\n",
      "|         E-Commerce|[2.16222073950649...| 2.162220739506491E9|-4.32648268498422...|1.159654297718094...|0.5349317056080334|4.120320881888916E10|\n",
      "|           Software|[1.85595799998956...| 1.855957999989563E9|-3.70932150071992...| 7.801318524147765E9| 0.660434615976291|3.971365925899658E10|\n",
      "|             Mobile|[1.56890610209523...| 1.568906102095231E9|-3.13735007452775...| 5.523512202221243E9|0.7083421147009048|3.184025170461084E10|\n",
      "|Enterprise Software|[1.54224322185118...|1.5422432218511863E9|-3.08611246774774...| 6.047339000459066E9| 0.661914776278602|2.921884039164990...|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQLQUERY =  \"\"\"\n",
    "            SELECT *\n",
    "            FROM summaryDF\n",
    "            ORDER BY Prediction DESC\n",
    "            \"\"\"\n",
    "#  ORDER BY YEAR DESC, TOTAL DESC\n",
    "\n",
    "sqlDF = spark.sql(SQLQUERY)\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
