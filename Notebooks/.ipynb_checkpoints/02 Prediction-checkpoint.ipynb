{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode,substring, length, udf\n",
    "from pyspark.sql.types import DecimalType, StringType\n",
    "from itertools import cycle\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is an custom SparkSession based on some config to work with Jupyter notebooks\n",
    "iv = spark.read.csv(\"hdfs://localhost:9000/user/lavish/data/investments.csv\"\n",
    "                , header='true'\n",
    "                , inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startYear=1995\n",
    "endYear=2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredIV = iv.filter(iv.raised_amount_usd.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "splittedCategoryIV = filteredIV.select('raised_amount_usd',  substring('funded_at',-4,4).cast('int').alias('year')\n",
    "                       , split(col(\"company_category_list\")\n",
    "                       , \"[|]s*\").alias(\"categoryArr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedIV=splittedCategoryIV.select('raised_amount_usd','year', explode('categoryArr').alias('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedIV.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedIV.createOrReplaceTempView(\"investments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM investments\")\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQUERY =  \"\"\"\n",
    "            SELECT CATEGORY, \n",
    "            CAST(YEAR AS INT), \n",
    "            SUM(RAISED_AMOUNT_USD) AS TOTAL, \n",
    "            CAST(SUM(RAISED_AMOUNT_USD) AS DECIMAL(30)) AS TOTAL_DEC \n",
    "            FROM INVESTMENTS GROUP \n",
    "            BY CATEGORY, YEAR \n",
    "            \"\"\"\n",
    "#  ORDER BY YEAR DESC, TOTAL DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(SQLQUERY)\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['YEAR'], outputCol = 'FEATURES')\n",
    "featureDF = vectorAssembler.transform(sqlDF).select('CATEGORY', 'FEATURES', 'TOTAL')\n",
    "\n",
    "featureDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = featureDF.select('CATEGORY').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topCategories = [row.CATEGORY for row in f.collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix to predict the amount for the Year 2020\n",
    "\n",
    "l =  [(2020,)]\n",
    "\n",
    "rdd = sc.parallelize(l)\n",
    "test = rdd.map(lambda x: Row(YEAR=x[0] ))\n",
    "testDF = sqlContext.createDataFrame(test)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['YEAR'], outputCol = 'FEATURES')\n",
    "vectorDF = vectorAssembler.transform(testDF).select('FEATURES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0 \n",
    "statInfo = {}\n",
    "for category in topCategories:\n",
    "    #print(category)\n",
    "    categoryDF=featureDF.filter(featureDF.CATEGORY == category)\n",
    "    if(categoryDF.count() > 10):\n",
    "        #count +=1\n",
    "        lr = LinearRegression(featuresCol = 'FEATURES', labelCol='TOTAL', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "        lr_model = lr.fit(categoryDF)\n",
    "        if (lr_model.summary.r2 >= .5):\n",
    "            statInfo[category] = { 'Gradient' : str(lr_model.coefficients),\n",
    "                                   'Intercept' : str(lr_model.intercept),\n",
    "                                   'RMSE' : str(lr_model.summary.rootMeanSquaredError),\n",
    "                                   'R2' : str(lr_model.summary.r2),\n",
    "                                   'Pediction' : lr_model.transform(vectorDF).take(1)[0].prediction                               }\n",
    "\n",
    "print (statInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
