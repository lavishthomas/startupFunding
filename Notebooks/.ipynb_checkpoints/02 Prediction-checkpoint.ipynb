{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode,substring, length, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from itertools import cycle\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is an custom SparkSession based on some config to work with Jupyter notebooks\n",
    "iv = spark.read.csv(\"hdfs://localhost:9000/user/lavish/data/investments.csv\"\n",
    "                , header='true'\n",
    "                , inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "startYear=1995\n",
    "endYear=2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company_permalink',\n",
       " 'company_name',\n",
       " 'company_category_list',\n",
       " 'company_country_code',\n",
       " 'company_state_code',\n",
       " 'company_region',\n",
       " 'company_city',\n",
       " 'investor_permalink',\n",
       " 'investor_name',\n",
       " 'investor_country_code',\n",
       " 'investor_state_code',\n",
       " 'investor_region',\n",
       " 'investor_city',\n",
       " 'funding_round_permalink',\n",
       " 'funding_round_type',\n",
       " 'funding_round_code',\n",
       " 'funded_at',\n",
       " 'raised_amount_usd']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iv.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredIV = iv.filter(iv.raised_amount_usd.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "splittedCategoryIV = filteredIV.select('raised_amount_usd',  substring('funded_at',-4,4).cast('int').alias('year')\n",
    "                       , split(col(\"company_category_list\")\n",
    "                       , \"[|]s*\").alias(\"categoryArr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedIV=splittedCategoryIV.select('raised_amount_usd','year', explode('categoryArr').alias('category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+-----------+\n",
      "|raised_amount_usd|year|   category|\n",
      "+-----------------+----+-----------+\n",
      "|        2000000.0|2008|Curated Web|\n",
      "|          41250.0|2014|      Games|\n",
      "|            2.0E7|2015|  Analytics|\n",
      "|        3000000.0|2013|  Analytics|\n",
      "|            2.0E7|2015|  Analytics|\n",
      "|        1700000.0|2013|  Analytics|\n",
      "|        8900000.0|2014|  Analytics|\n",
      "|            2.0E7|2015|  Analytics|\n",
      "|            2.0E7|2015|  Analytics|\n",
      "|        8900000.0|2014|  Analytics|\n",
      "+-----------------+----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodedIV.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedIV.createOrReplaceTempView(\"investments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+-----------+\n",
      "|raised_amount_usd|year|   category|\n",
      "+-----------------+----+-----------+\n",
      "|        2000000.0|2008|Curated Web|\n",
      "|          41250.0|2014|      Games|\n",
      "|            2.0E7|2015|  Analytics|\n",
      "|        3000000.0|2013|  Analytics|\n",
      "|            2.0E7|2015|  Analytics|\n",
      "+-----------------+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM investments\")\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLQUERY =  \"\"\"\n",
    "            SELECT CATEGORY, \n",
    "            CAST(YEAR AS INT), \n",
    "            SUM(RAISED_AMOUNT_USD) AS TOTAL, \n",
    "            CAST(SUM(RAISED_AMOUNT_USD) AS DECIMAL(30)) AS TOTAL_DEC \n",
    "            FROM INVESTMENTS GROUP \n",
    "            BY CATEGORY, YEAR \n",
    "            \"\"\"\n",
    "#  ORDER BY YEAR DESC, TOTAL DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+-----------------+----------+\n",
      "|          CATEGORY|YEAR|            TOTAL| TOTAL_DEC|\n",
      "+------------------+----+-----------------+----------+\n",
      "|    Interest Graph|2011|           3.28E7|  32800000|\n",
      "|         Insurance|2015|  5.70529580149E9|5705295801|\n",
      "|Big Data Analytics|2013|     2.35683979E9|2356839790|\n",
      "|         Aerospace|2014|4.6013734510098E8| 460137345|\n",
      "|             Audio|2005|          1.058E8| 105800000|\n",
      "+------------------+----+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(SQLQUERY)\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+-----------------+\n",
      "|          CATEGORY|FEATURES|            TOTAL|\n",
      "+------------------+--------+-----------------+\n",
      "|    Interest Graph|[2011.0]|           3.28E7|\n",
      "|         Insurance|[2015.0]|  5.70529580149E9|\n",
      "|Big Data Analytics|[2013.0]|     2.35683979E9|\n",
      "|         Aerospace|[2014.0]|4.6013734510098E8|\n",
      "|             Audio|[2005.0]|          1.058E8|\n",
      "+------------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['YEAR'], outputCol = 'FEATURES')\n",
    "featureDF = vectorAssembler.transform(sqlDF).select('CATEGORY', 'FEATURES', 'TOTAL')\n",
    "\n",
    "featureDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = featureDF.select('CATEGORY').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topCategories = [row.CATEGORY for row in f.collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix to predict the amount for the Year 2020\n",
    "\n",
    "l =  [(2020,)]\n",
    "\n",
    "rdd = sc.parallelize(l)\n",
    "test = rdd.map(lambda x: Row(YEAR=x[0] ))\n",
    "testDF = sqlContext.createDataFrame(test)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['YEAR'], outputCol = 'FEATURES')\n",
    "vectorDF = vectorAssembler.transform(testDF).select('FEATURES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0 \n",
    "summaryInfo = []\n",
    "columns = ['Category', 'Gradient', 'Intercept','RMSE', 'R2', 'Pediction']\n",
    "\n",
    "statInfo = []\n",
    "for category in topCategories:\n",
    "    #print(category)\n",
    "    categoryDF=featureDF.filter(featureDF.CATEGORY == category)\n",
    "    if(categoryDF.count() > 10):\n",
    "        #count +=1\n",
    "        lr = LinearRegression(featuresCol = 'FEATURES', labelCol='TOTAL', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "        lr_model = lr.fit(categoryDF)\n",
    "        if (lr_model.summary.r2 >= .5):\n",
    "            row=(category\n",
    "                 ,lr_model.coefficients\n",
    "                 ,lr_model.intercept\n",
    "                 ,lr_model.summary.rootMeanSquaredError\n",
    "                 ,lr_model.summary.r2\n",
    "                 ,lr_model.transform(vectorDF).take(1)[0].prediction )\n",
    "            summaryInfo.append(row)                        \n",
    "summaryDF = spark.createDataFrame(summaryInfo, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|            Category|            Gradient|           Intercept|                RMSE|                R2|           Pediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|    Personal Finance|[3.09531844542449...|-6.20846579526586...| 1.140333317135928E9|0.5257188832329348| 4.407746449161743E9|\n",
      "|Reviews and Recom...|[9.38435051052876...|-1.88076601007935...|2.2526002798534468E8|0.6748252913533789|1.4872793047452393E9|\n",
      "|         Health Care|[1.08986421814419E9]|-2.17925088637451...|2.0237455217204463E9|0.8747611056595241|2.227483427674951E10|\n",
      "|Application Perfo...|[1.46386390370918...|-2.93690139320354...|4.7040592338190275E8|0.5668998218851391| 2.010369228901245E9|\n",
      "|        Credit Cards|[6.33419228608998...|-1.27009681219964...|2.5974312104301742E8|0.5027780036421106| 9.410029590531769E8|\n",
      "|       Collaboration|[2.88729181304805...|-5.78680486883181...|1.1126428185751677E9|0.5577629546360174| 4.552459352525635E9|\n",
      "|               Video|[2.98971570988925...|-5.98228194300554...|1.1313897051071873E9|0.6270858327162598| 5.694379097075684E9|\n",
      "|   Retail Technology|[5.09435917649277...|-1.02077339130405...|2.0878675224244386E8|0.5065814877897405| 8.287162347489319E8|\n",
      "|              EdTech|[2.06764166217049...|-4.14113543353799...| 8.128661690529667E8|0.6253133179808209| 3.550072404640991E9|\n",
      "|                 SEO|[3.28335373355263...|-6.56729294713806...|1.3925710858168542E8|0.5567569012102666| 6.508159463824539E8|\n",
      "|            Security|[3.69197116124124...|-7.37928916987564...|1.7655355198807151E9|0.6017134099240016| 7.849257583166138E9|\n",
      "|     Brand Marketing|[1.08303617187889...|-2.16864846254352...|3.7368473615189695E8|0.6773067149915923|1.9084604651853027E9|\n",
      "|             Fashion|[5.06023941051812...|-1.01392455132537...|1.6144677184628706E9|0.6719792518791303| 8.243809599288696E9|\n",
      "|                 iOS|[1.33775193841834...|-2.68324351675469...| 3.109009683732537E8|0.7182528190869092| 1.901539885035492E9|\n",
      "|    Machine Learning|[3.90223939089104...|-7.83200461885002...|1.0819664029209907E9|0.5892549781300859| 5.051895074988892E9|\n",
      "|          Accounting|[5.90140786323025...|-1.18233689472827...| 1.940194529923822E8|0.6406420426161794| 9.747493644239502E8|\n",
      "|              iPhone|[1.36587079837824...|-2.73559386843978...|   3.3417803375328E8|0.7672261239183228|2.3465144284259033E9|\n",
      "|     Cloud Computing|[6.58598130460782...|-1.31892577507840...|2.3820565736397595E9|0.6479952335985499|1.144244845237426...|\n",
      "|       IT Management|[3.44475677072277...|-6.90279272235049...|1.1933907524279033E8|0.5384232190145429| 5.561595450950317E8|\n",
      "|Marketing Automation|[1.19724126876351...|-2.39793201618893...| 5.574795854488145E8|0.5245369647977931|2.0495346713373413E9|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrameWriter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d1a7cbb7b4f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummaryDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'summaryDF.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrameWriter' object is not callable"
     ]
    }
   ],
   "source": [
    "summaryDF.write.('summaryDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDF.write.saveAsTable(\"summaryDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|            Category|            Gradient|           Intercept|                RMSE|                R2|           Pediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|                Kids|[3.18674767862354...|-6.39071308213002...| 9.505712557482366E7|0.5291670832619793| 4.651722868952179E8|\n",
      "|Location Based Se...|[5.78892288743861...|-1.15886232468249...|1.6588587121286646E8|0.7474126418043531|1.0500098580102386E9|\n",
      "|           Lifestyle|[1.34058267510281...|-2.68492723540129...| 5.407734746394048E8|0.5876254467293467| 2.304976830638977E9|\n",
      "|Mobile Software T...|[6.86504849357692...|-1.37568550275783...|2.2023283087968177E8|0.6460663909210812|1.1054292944698792E9|\n",
      "|             Storage|[3.37263853573364...|-6.75052385972356...|1.4688655636029084E9|0.5292015214853756| 6.220598245840698E9|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQLQUERY =  \"\"\"\n",
    "            SELECT*\n",
    "            FROM summaryDF\n",
    "            \n",
    "            \"\"\"\n",
    "#  ORDER BY YEAR DESC, TOTAL DESC\n",
    "\n",
    "sqlDF = spark.sql(SQLQUERY)\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
